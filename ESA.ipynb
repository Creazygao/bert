{"cells":[{"cell_type":"markdown","metadata":{"id":"DvdIhmlTUjqj"},"source":["#预训练BERT"]},{"cell_type":"markdown","metadata":{"id":"qsmOO4N6Uun6"},"source":["##下载wiki原始数据集"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5euEorVvU0pe"},"outputs":[],"source":["#zhwiki.xml.bz2"]},{"cell_type":"markdown","metadata":{"id":"Lgdp24T4U6uH"},"source":["##使用wikiextractor进行初步清洗、分隔"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"147z7rF6UMVG"},"outputs":[],"source":["!pip install wikiextractor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uygDVVdiVEhf"},"outputs":[],"source":["\n","##处理wiki中文原始数据\n","#cd /content/drive/MyDrive/wiki_data/\n","#python -m wikiextractor.WikiExtractor -b 50M zhwiki.xml.bz2"]},{"cell_type":"markdown","metadata":{"id":"_GTT0OTRVWXW"},"source":["##安装zhconv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXGqy3HlVkPo"},"outputs":[],"source":["!pip install zhconv"]},{"cell_type":"markdown","metadata":{"id":"NanvQ-ZJWE1E"},"source":["##导入依赖"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-JCKU12VpK3"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from datetime import datetime\n","import pandas as pd\n","import numpy as np\n","import re\n","from tensorflow.core.example.feature_pb2 import Features\n","from typing import List\n","from datetime import datetime\n","from typing import Counter\n","import zhconv\n","import copy\n","import tensorflow_addons as tfa"]},{"cell_type":"markdown","metadata":{"id":"r0DQOvtRWG2b"},"source":["##生成bert词表"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AitSSrGEWPgn"},"outputs":[],"source":["def get_vocb(file_path, vocb_path, min_freq):\n","  '''\n","  使用file_path文件生成vocb_path词表，min_freq为最小词频\n","  '''\n","    vocabulary2 = \"\"\n","    if os.path.exists(vocb_path):\n","        with open(vocb_path, 'r', encoding='utf-8') as f:\n","            vocabulary2 = eval(f.read())\n","    elif os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = f.read();\n","        res = re.compile(\"[^\\\\u4e00-\\\\u9fa5^a-z^A-Z^0-9]\")\n","        data = res.sub('', data)\n","        words_freq = Counter(data).most_common()\n","        vocabulary2 = [words for (words, freq) in words_freq if freq > min_freq]\n","        vocabulary2 = ['CLS', 'SEP', 'MASK', 'PAD', 'UNK'] + vocabulary2\n","        with open(vocb_path, 'w', encoding='utf-8') as f:\n","            f.write(str(vocabulary2))\n","    vocb2id_dic = dict(zip(vocabulary2, list(range(len(vocabulary2)))))\n","    id2vocb_dic = dict(zip(list(range(len(vocabulary2))), vocabulary2))\n","    length = len(vocabulary2)\n","    return vocb2id_dic, id2vocb_dic, length"]},{"cell_type":"markdown","metadata":{"id":"4821prLHWWri"},"source":["##去除特殊字符、繁简体转换、生成token、mask、句子对，返回tfrecord"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cn8gLtPUWI2O"},"outputs":[],"source":["\n","def _bytes_feature(value):\n","  '''\n","  字符串特征格式\n","  '''\n","    if isinstance(value, type(tf.constant(0))):\n","        value = value.numpy()\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def _float_feature(value):\n","    '''\n","  浮点数特征格式\n","  '''\n","    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n","\n","\n","def _int64_feature(value):\n","  '''\n","  整数型特征格式\n","  '''\n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n","\n","\n","def get_example_data(f1, f2, f3, f4, f5, f6):\n","  '''\n","  返回tf.train.example作为后续惰性加载数据\n","  '''\n","    example = tf.train.Example(features=tf.train.Features(feature={\n","        \"label_y\": _float_feature([f1]),\n","        \"raw_token\": _float_feature(\n","            f2),\n","        \"segment_token\": _float_feature(\n","            f3),\n","        \"mask_token\": _float_feature(\n","            f4),\n","        \"mask_pos\": _float_feature(f5),\n","        \"pad_token\": _float_feature(f6),\n","    }))\n","    return example\n","\n","\n","def get_record_data(sentences1, sentences2, word_to_id, seq_len, mask_rate):\n","  '''\n"," 生成tfrecord格式数据\n","  '''\n","    s_len = 0\n","    sep_token = word_to_id[\"SEP\"]\n","    cls_token = word_to_id[\"CLS\"]\n","\n","    res = re.compile(\"[^\\\\u4e00-\\\\u9fa5^a-z^A-Z^0-9]\")\n","    sentence1 = res.sub('', sentences1)\n","    sentence2 = res.sub('', sentences2)\n","    if len(sentence1) > seq_len - 2:\n","        sentence1 = sentence1[:seq_len - 2]\n","    if len(sentence2) > seq_len - 2:\n","        sentence2 = sentence2[:seq_len - 2]\n","    # 生成句子的token\n","    token1 = []\n","    token2 = []\n","    exam = []\n","    for v in sentence1:\n","        if v in word_to_id:\n","            token1.append(word_to_id[v])\n","        else:\n","            token1.append(word_to_id['UNK'])\n","    for v in sentence2:\n","        if v in word_to_id:\n","            token2.append(word_to_id[v])\n","        else:\n","            token2.append(word_to_id['UNK'])\n","    sentence1_token = [cls_token] + token1 + [sep_token] + token2 + [sep_token]\n","    sentence2_token = [cls_token] + token2 + [sep_token] + token1 + [sep_token]\n","    pad1_token=[]\n","    #生成token、padding\n","    if len(sentence1_token) < seq_len:\n","        pad1_token = [0] * len(sentence1_token) + [1] * (seq_len - len(sentence1_token))\n","        s_len = len(sentence1_token) - 2\n","        sentence1_token += [word_to_id['PAD']] * (seq_len - len(sentence1_token))\n","        sentence2_token += [word_to_id['PAD']] * (seq_len - len(sentence2_token))\n","       \n","\n","    elif len(sentence1_token) > seq_len:\n","        s_len = seq_len - 2\n","        sentence1_token = sentence1_token[0:seq_len - 1]\n","        sentence1_token.extend([word_to_id[\"SEP\"]])\n","        sentence2_token = sentence2_token[0:seq_len - 1]\n","        sentence2_token.extend([word_to_id[\"SEP\"]])\n","        pad1_token = [0] * seq_len\n","\n","    pad2_token = pad1_token\n","    #生成segment padding\n","    segment1_token = [0] * (len(sentence1) + 2) + [1] * (seq_len - len(sentence1) - 2)\n","    segment2_token = [0] * (len(sentence2) + 2) + [1] * (seq_len - len(sentence2) - 2)\n","    #生成mask，多次重复生成，模仿动态mask\n","    for i in range(s_len // 50):\n","        mask_num = np.ceil(s_len * mask_rate).astype(int)\n","        position1 = np.random.choice(a=np.arange(1, s_len + 1), size=mask_num, replace=False)\n","        position2 = np.random.choice(a=np.arange(1, s_len + 1), size=mask_num, replace=False)\n","        mask1_token = copy.deepcopy(sentence1_token)\n","        mask2_token = copy.deepcopy(sentence2_token)\n","        mask1_pos = [0] * seq_len\n","        mask2_pos = [0] * seq_len\n","        for p in position1:\n","            mask1_token[p] = word_to_id[\"MASK\"]\n","            mask1_pos[p] = 1\n","        for p in position2:\n","            mask2_token[p] = word_to_id[\"MASK\"]\n","            mask2_pos[p] = 1\n","        example1 = get_example_data(1, sentence1_token, segment1_token, mask1_token, mask1_pos, pad1_token)\n","        example2 = get_example_data(0, sentence2_token, segment2_token, mask2_token, mask2_pos, pad2_token)\n","        exam.append(example1)\n","        exam.append(example2)\n","    return exam\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2lPsNvq-X4xw"},"source":["##清洗数据、繁简转换、去除特殊字符"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fT2UVhFX6n_"},"outputs":[],"source":["def clean_data(file_path, target_file, word_to_id, seq_len, mask_rate):\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = f.readlines()\n","        writer = tf.io.TFRecordWriter(target_file)\n","        for line in data:\n","            new_line = zhconv.convert(line, 'zh-cn')\n","            sentences = new_line.strip('\\n').strip('。').split('。')\n","            if len(sentences) < 2:\n","                continue\n","            for i in range(len(sentences) - 1):\n","                tempdata = get_record_data(sentences[i], sentences[i + 1], word_to_id, seq_len, mask_rate)\n","                for val in tempdata:\n","                    writer.write(val.SerializeToString())\n","        f.close()\n","        writer.close()\n","    else:\n","        print(\"no such file or dir\")\n","        return"]},{"cell_type":"markdown","metadata":{"id":"Vcl6CTa8YIe3"},"source":["##定义embedding层"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4al0WzWYQy2"},"outputs":[],"source":["\n","class Embedding_layer(tf.keras.layers.Layer):\n","    def __init__(self, vocb_size, embedding_size=512, max_seq_len=128, segment_size=2, dropout_prob=0.0, **kwargs):\n","        super(Embedding_layer, self).__init__(**kwargs)\n","        self.vocb_size = vocb_size\n","        self.embedding_size = embedding_size\n","        self.max_seq_len = max_seq_len\n","        self.segment_size = segment_size\n","        self.dropout_prob = dropout_prob\n","\n","    def build(self, input_shape):\n","        self.token_embedding = tf.keras.layers.Embedding(input_dim=self.vocb_size,\n","                                                         output_dim=self.embedding_size,\n","                                                         embeddings_initializer=tf.keras.initializers.TruncatedNormal(),\n","                                                         dtype=tf.float32, name=\"layers1\")\n","        self.segment_embedding = tf.keras.layers.Embedding(input_dim=self.segment_size, output_dim=self.embedding_size,\n","                                                           embeddings_initializer=tf.keras.initializers.TruncatedNormal(),\n","                                                           dtype=tf.float32, name=\"layers2\")\n","        #使用固定位置编码，暂时未实现其他位置编码\n","        self.positional_embedding = self.add_weight(shape=(self.max_seq_len, self.embedding_size),\n","                                                    initializer=tf.keras.initializers.TruncatedNormal(),\n","                                                    dtype=tf.float32, name=\"layers4\")\n","        self.output_layer_norm = tf.keras.layers.LayerNormalization()\n","        self.output_dropout = tf.keras.layers.Dropout(self.dropout_prob)\n","        super(Embedding_layer, self).build(input_shape)"]},{"cell_type":"markdown","metadata":{"id":"SowUVIbsYU8-"},"source":["定义attention计算"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGz-TOXeYbU9"},"outputs":[],"source":["def attention(q, k, v, mask):\n","  '''\n","  通过使用-1e9，使mask位置置零\n","  '''\n","    matmul_qk = tf.matmul(q, k, transpose_b=True) \n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","    if mask is not None:\n","        scaled_attention_logits + (tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=tf.float32) * -1e9)\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"EznUtndFY3CS"},"source":["##multi_heads attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7MI8pZEY6wG"},"outputs":[],"source":["\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","  '''\n","  严格意义上，并不清除分为多头的意义何在，cnn分为多头是多了参数量，\n","  但是在该处，参数量并没有增加，只是在维度上的运算\n","  '''\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, x, mask):\n","        batch_size = tf.shape(x)[0]\n","        query = self.wq(x)  \n","        key = self.wk(x)  \n","        value = self.wv(x)  \n","        query = self.split_heads(query, batch_size)  \n","        key = self.split_heads(key, batch_size) \n","        value = self.split_heads(value, batch_size)  \n","        scaled_attention, attention_weights = attention(query, key, value, mask)\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","        return output\n"]},{"cell_type":"markdown","metadata":{"id":"SxxXGe3hZOnM"},"source":["##transformer的encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oju4VfcQZXJy"},"outputs":[],"source":["class Transformer(tf.keras.layers.Layer):\n","  '''\n","  多头注意力+layernorm+残差+dense+dense\n","  '''\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation='relu'),\n","            tf.keras.layers.Dense(d_model) \n","        ])\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, mask, training=None):\n","        attn_output = self.mha(x, mask)  \n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output) \n","\n","        ffn_output = self.ffn(out1) \n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out = self.layernorm2(out1 + ffn_output) \n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"-eZ6M9koZssv"},"source":["##定义bert结构"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3_4QPYFZu31"},"outputs":[],"source":["class Bert(tf.keras.Model):\n","  '''\n","  embedding+多个transformer，最后输出3项结果用于不同任务\n","  '''\n","    def __init__(self, vocab_size, embedding_size, max_seq_len, segment_size, num_transformer_layers,\n","                 num_attention_heads, intermediate_size, **kwargs):\n","        super(Bert, self).__init__(**kwargs)\n","        self.vocab_size = vocab_size\n","        self.embedding_size = embedding_size\n","        self.max_seq_len = max_seq_len\n","        self.segment_size = segment_size\n","        self.num_transformer_layers = num_transformer_layers\n","        self.num_attention_heads = num_attention_heads\n","        self.intermediate_size = intermediate_size\n","        self.embedding = Embedding_layer(vocb_size=self.vocab_size, embedding_size=self.embedding_size,\n","                                         max_seq_len=self.max_seq_len,\n","                                         segment_size=self.segment_size, )\n","        self.transformer1 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer2 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer3 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer4 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer5 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer6 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer7 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.transformer8 = Transformer(d_model=self.embedding_size, num_heads=self.num_attention_heads,\n","                                               dff=self.intermediate_size)\n","        self.nsp_predictor = tf.keras.layers.Dense(2)\n","\n","    def call(self, inputs, training=None):\n","        batch_x, batch_mask, batch_segment = inputs\n","        x = self.embedding((batch_x, batch_segment))\n","        x = self.transformer1(x, mask=batch_mask, training=training)\n","        x = self.transformer2(x, mask=batch_mask, training=training)\n","        x = self.transformer3(x, mask=batch_mask, training=training)\n","        x = self.transformer4(x, mask=batch_mask, training=training)\n","        x = self.transformer5(x, mask=batch_mask, training=training)\n","        x = self.transformer6(x, mask=batch_mask, training=training)\n","        x = self.transformer7(x, mask=batch_mask, training=training)\n","        x = self.transformer8(x, mask=batch_mask, training=training)\n","        first_token_tensor = x[:, 0, :]\n","        is_next_predict = self.nsp_predictor(first_token_tensor)\n","        word_mask_predict = tf.matmul(x, self.embedding.token_embedding.embeddings, transpose_b=True)\n","        sequence_output = x\n","\n","        return is_next_predict, word_mask_predict, sequence_output"]},{"cell_type":"markdown","metadata":{"id":"BjvhP19_aBp8"},"source":["##定义loss、metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5dvXdNHaKtV"},"outputs":[],"source":["class BERT_Loss(tf.keras.layers.Layer):\n","'''\n","loss=mask_loss+next_sentences_loss\n","'''\n","    def __init__(self):\n","        super(BERT_Loss, self).__init__()\n","\n","    def call(self, inputs):\n","        (mlm_predict, batch_mlm_mask, origin_x, nsp_predict, batch_y) = inputs\n","        x_pred = tf.nn.softmax(mlm_predict, axis=-1)\n","        mlm_loss = tf.keras.losses.sparse_categorical_crossentropy(origin_x, x_pred)\n","        mlm_loss = tf.math.reduce_sum(mlm_loss * batch_mlm_mask, axis=-1) / (\n","                    tf.math.reduce_sum(batch_mlm_mask, axis=-1) + 1)\n","        y_pred = tf.nn.softmax(nsp_predict, axis=-1)\n","        nsp_loss = tf.keras.losses.sparse_categorical_crossentropy(batch_y, y_pred)\n","\n","        return nsp_loss, mlm_loss\n","\n","\n","\n","def calculate_pretrain_task_accuracy(nsp_predict, mlm_predict, batch_mlm_mask, origin_x, batch_y):\n","  '''\n","  使用loss和acc作为每个batch的指标\n","  '''\n","    y_predict = tf.math.argmax(nsp_predict, axis=-1)\n","    nsp_accuracy = tf.keras.metrics.Accuracy()\n","    nsp_accuracy.update_state(y_predict, batch_y)\n","    nsp_accuracy = nsp_accuracy.result().numpy()\n","\n","    batch_mlm_mask = tf.cast(batch_mlm_mask, dtype=tf.int32)\n","    index = tf.where(batch_mlm_mask == 1)\n","    x_predict = tf.math.argmax(mlm_predict, axis=-1)\n","    x_predict = tf.gather_nd(x_predict, index)\n","    x_real = tf.gather_nd(origin_x, index)\n","    mlm_accuracy = tf.keras.metrics.Accuracy()\n","    mlm_accuracy.update_state(x_predict, x_real)\n","    mlm_accuracy = mlm_accuracy.result().numpy()\n","\n","    return nsp_accuracy, mlm_accuracy"]},{"cell_type":"markdown","metadata":{"id":"9ikAniM3aYZB"},"source":["##定义预训练过程"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcVQBytpak_Q"},"outputs":[],"source":["#加载词表\n","w_i, i_w, vocb_len = get_vocb(\"/content/drive/MyDrive/wiki_data/wiki_00\", \"/content/drive/MyDrive/wiki_data/bert_vocb.txt\", 1)\n","#清洗数据，生成tfrecord\n","clean_data(\"/content/drive/MyDrive/wiki_data/wiki_00\", \"/content/drive/MyDrive/wiki_data/bert.tfrecords\", w_i, 128, 0.10)\n","#构建数据解码器\n","def decode_fn(record_bytes):\n","    feature_map = {\n","                   \"label_y\": tf.io.FixedLenFeature([1], dtype=tf.float32),\n","                   \"raw_token\": tf.io.FixedLenFeature([128], dtype=tf.float32),\n","                   \"segment_token\": tf.io.FixedLenFeature([128], dtype=tf.float32),\n","                   \"mask_token\": tf.io.FixedLenFeature([128], dtype=tf.float32),\n","                   \"mask_pos\": tf.io.FixedLenFeature([128], dtype=tf.float32),\n","                   \"pad_token\": tf.io.FixedLenFeature([128], dtype=tf.float32)\n","                   }\n","    tempdata = tf.io.parse_single_example(record_bytes, features=feature_map)\n","    return tempdata\n","#加载数据\n","data_set = tf.data.TFRecordDataset([\"/content/drive/MyDrive/wiki_data/bert.tfrecords\"]).shuffle(buffer_size=1000,reshuffle_each_iteration=True)\n","data_use = data_set.map(decode_fn)\n","data_use = data_use.batch(96)\n","#定义bert超参数\n","vocab_size = vocb_len\n","print(vocab_size)\n","embedding_size = 512\n","max_seq_len = 512\n","segment_size = 2\n","num_transformer_layers = 12\n","num_attention_heads = 12\n","intermediate_size = 2048\n","#初始化bert\n","model = Bert(vocab_size, embedding_size, max_seq_len,\n","             segment_size, num_transformer_layers, num_attention_heads, intermediate_size, )\n","optimizer = tfa.optimizers.LAMB(learning_rate=5e-4)\n","loss_fn = BERT_Loss()\n","#管理checkpoint\n","checkpoint = tf.train.Checkpoint(model=model)\n","checkpoint.restore(tf.train.latest_checkpoint(\"/content/drive/MyDrive/wiki_data/model\"))\n","manager = tf.train.CheckpointManager(checkpoint, directory=\"/content/drive/MyDrive/wiki_data/model\", max_to_keep=5)\n","#自定义训练过程\n","EPOCH = 100\n","for epoch in range(EPOCH):\n","    it = iter(data_use)\n","    for step in range(100000):    \n","        mydata = next(it, None)\n","        if mydata == None:\n","            break\n","        batch_x = mydata[\"mask_token\"]\n","        batch_mlm_mask =mydata[\"mask_pos\"]\n","        origin_x = mydata[\"raw_token\"]\n","        batch_segment = mydata[\"segment_token\"]\n","        batch_padding_mask = mydata[\"pad_token\"]\n","        batch_y = mydata[\"label_y\"]\n","        with tf.GradientTape() as t:\n","            nsp_predict, mlm_predict, sequence_output = model((batch_x, batch_padding_mask, batch_segment),training=True)\n","            nsp_loss, mlm_loss = loss_fn((mlm_predict, batch_mlm_mask, origin_x, nsp_predict, batch_y))\n","            nsp_loss = tf.reduce_mean(nsp_loss)\n","            mlm_loss = tf.reduce_mean(mlm_loss)\n","            loss = nsp_loss + mlm_loss\n","        gradients = t.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        nsp_acc, mlm_acc = calculate_pretrain_task_accuracy(nsp_predict, mlm_predict, batch_mlm_mask, origin_x, batch_y)\n","        #每50步输出一次训练信息\n","        if step % 50 == 0:\n","            print(\n","                'Epoch {}, step {}, loss {:.4f}, mask_loss {:.4f}, mask_acc {:.4f}, next_sentence_loss {:.4f}, next_sentence_acc {:.4f}'.format(\n","                    epoch, step, loss.numpy(),\n","                    mlm_loss.numpy(),\n","                    mlm_acc,\n","                    nsp_loss.numpy(), nsp_acc\n","                ))\n","    #每个epoch保存一次模型\n","    path = manager.save(checkpoint_number=epoch)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNDUl31OkulprqpHdye+RJS","collapsed_sections":[],"name":"8-8-512bert预训练.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
